# SecureTranscribe Testing Guide

This guide provides comprehensive testing tools to verify that your transcription and diarization processing pipeline works correctly, especially focusing on the queue management and status tracking issues you've experienced.

## Quick Start

### 1. Run Comprehensive Tests
```bash
cd SecureTranscribe
python3 scripts/run_tests.py
```

### 2. Run Pipeline Diagnostics (Recommended First Step)
```bash
python3 scripts/diagnose_pipeline.py
```

### 3. Quick Test for Fast Feedback
```bash
python3 scripts/run_tests.py --quick
```

## Problem Diagnosis

The tests are specifically designed to help identify issues where:

- Jobs disappear from the queue during processing
- Web interface shows empty queue when jobs are processing
- Transcription never completes
- Status updates aren't propagated correctly
- Progress tracking isn't working

## What the Tests Check

### 1. Environment & Configuration
- âœ… Required environment variables are set correctly
- âœ… Directory permissions and structure
- âœ… Database connectivity
- âœ… Import dependencies

### 2. Queue Management
- âœ… Jobs are properly queued
- âœ… Status transitions work: queued â†’ processing â†’ completed
- âœ… Queue statistics are accurate
- âœ… Worker pool status
- âœ… Database consistency during processing

### 3. API Endpoints
- âœ… All endpoints respond correctly
- âœ… Session management works
- âœ… File upload process
- âœ… Status reporting endpoints
- âœ… Error handling

### 4. Processing Pipeline
- âœ… File upload to transcription start
- âœ… Progress callback functionality
- âœ… Mock GPU services (works without CUDA)
- âœ… Error handling and recovery
- âœ… Concurrent job processing

### 5. Status Tracking
- âœ… Real-time status updates via API
- âœ… Progress percentage tracking
- âœ… Database state consistency
- âœ… Queue position calculations
- âœ… Wait time estimations

## Running Specific Tests

### Diagnose Specific Issues

**If jobs disappear from queue:**
```bash
python3 scripts/diagnose_pipeline.py
```
Focus on:
- Queue service status
- Database consistency checks
- Recent job entries

**If web interface shows empty queue:**
```bash
python3 scripts/run_tests.py --specific tests/integration/test_processing_pipeline.py::TestProcessingPipeline::test_queue_status_tracking
```

**If transcription never completes:**
```bash
python3 scripts/run_pipeline_tests.py
```
This runs with mocked GPU services to isolate the issue.

**If progress tracking isn't working:**
```bash
python3 scripts/run_tests.py --specific tests/integration/test_processing_pipeline.py::TestProcessingPipeline::test_progress_callback_functionality
```

## Test Environment

The tests automatically configure a safe testing environment:
```bash
TEST_MODE=true              # Enable test-specific behaviors
MOCK_GPU=true              # Use mocked GPU services (no CUDA needed)
CUDA_VISIBLE_DEVICES=        # Disable GPU detection
LOG_LEVEL=DEBUG             # Detailed logging for debugging
ALLOWED_HOSTS=["localhost", "127.0.0.1"]
CORS_ORIGINS=["http://localhost:3000"]
```

## Expected Test Results

### Successful Test Run
```
ğŸš€ SecureTranscribe Master Test Runner
============================================================
ğŸ“… Started: 2024-01-15 10:30:00
ğŸ“ Project Root: /path/to/SecureTranscribe
============================================================

ğŸ§ª Checking Dependencies...
âœ… fastapi
âœ… sqlalchemy
âœ… pydantic
âœ… All required dependencies are available

ğŸ§ª Running: Pipeline Diagnostics
ğŸ’» Command: python3 scripts/diagnose_pipeline.py
============================================================
ğŸ” Environment Check
âœ… DATABASE_URL: sqlite:///./securetranscribe.db
âœ… UPLOAD_DIR: ./uploads
âœ… LOG_LEVEL: DEBUG
âœ… All required environment variables set

ğŸ” Import Check
âœ… main - FastAPI application
âœ… app.core.config - Configuration module
âœ… All modules imported successfully

ğŸ” Database Check
âœ… Database connection successful
âœ… processing_queue: 0 records
âœ… Database is accessible

ğŸ” Queue Service Check
âœ… Queue service instantiated successfully
ğŸ“Š Total jobs: 0
ğŸ“Š Queued jobs: 0
ğŸ“Š Processing jobs: 0
âœ… Queue service is running

âœ… PASS: Pipeline Diagnostics

ğŸ§ª Running: Integration Tests
ğŸ’» Command: python3 scripts/run_pipeline_tests.py
ğŸš€ Starting SecureTranscribe Pipeline Tests
âœ… Created test audio file: /tmp/tmpXXXXXX/test_audio.wav
âœ… All modules imported successfully
âœ… Health check passed
âœ… Session created: abc-123-def-456
âœ… File uploaded: 789
âœ… Transcription started successfully
  ğŸ“Š Status: processing, Progress: 25%, Step: Loading audio...
  ğŸ“Š Status: processing, Progress: 50%, Step: Transcribing...
  ğŸ“Š Status: processing, Progress: 75%, Step: Finalizing results...
âœ… Transcription completed successfully
âœ… Queue status tracking is working correctly
============================================================
ğŸ“Š TEST SUMMARY
============================================================
âœ… PASS: Module Imports
âœ… PASS: Basic Functionality
âœ… PASS: File Upload
âœ… PASS: Mock Processing
âœ… PASS: Progress Updates
âœ… PASS: Queue Status Tracking
--------------------------------------------------
Total: 6/6 tests passed
ğŸ‰ All tests passed! Your processing pipeline is working correctly.
âœ… PASS: Integration Tests

============================================================
ğŸ“Š FINAL TEST REPORT
============================================================
ğŸ“… Completed: 2024-01-15 10:32:15
â±ï¸  Duration: 135.42 seconds
ğŸ“ˆ Results: 4/4 test suites passed
  âœ… PASS: dependencies
  âœ… PASS: environment_setup
  âœ… PASS: diagnostics
  âœ… PASS: integration_tests
ğŸ¯ Overall Status: PASSED

ğŸ‰ All tests passed! Your SecureTranscribe pipeline is working correctly.
```

## Troubleshooting Failed Tests

### Common Issues and Solutions

**Import Errors:**
```
âŒ Import failed: No module named 'app.main'
```
- Ensure you're in project root directory
- Activate virtual environment: `source venv/bin/activate`
- Install dependencies: `pip install -r requirements.txt`

**Permission Errors:**
```
âŒ ./uploads does not exist
```
- Create directories: `mkdir -p uploads processed logs`
- Check permissions: `chmod 755 uploads processed logs`

**Database Issues:**
```
âŒ Database connection failed
```
- Check file permissions: `ls -la securetranscribe.db`
- Ensure directory is writable: `chmod 755 .`

**GPU/CUDA Issues:**
- Tests automatically use `MOCK_GPU=true`
- No CUDA required for testing
- For production GPU issues, check GPU service configuration

**API Endpoint Failures:**
```
âŒ GET /api/queue/status: HTTP 500
```
- Check application logs: `tail -f logs/securetranscribe.log`
- Ensure application is running: `uvicorn app.main:app --reload`

## Test Scripts Overview

### `scripts/run_tests.py` - Master Test Runner
Main entry point for all testing. Use this for comprehensive testing.

**Options:**
- `--quick` - Fast test suite for development
- `--diagnostics-only` - Run only diagnostic checks
- `--specific <path>` - Run specific test file/method
- `--skip-diagnostics` - Skip diagnostic checks
- `--skip-unit` - Skip unit tests

### `scripts/diagnose_pipeline.py` - Pipeline Diagnostics
Best first step when troubleshooting issues. Checks:
- Environment configuration
- Module imports
- Database connectivity
- Queue service status
- API functionality
- File upload workflow
- GPU service configuration
- Recent log analysis

### `scripts/run_pipeline_tests.py` - Integration Tests
Standalone integration tests with mocked GPU services. Tests:
- Complete transcription workflow
- Queue status tracking
- Progress callbacks
- Error handling
- Database consistency
- Concurrent processing

## Development Workflow Integration

### Before Committing Changes
```bash
python3 scripts/run_tests.py --quick
```

### Before Merging to Main
```bash
python3 scripts/run_tests.py
```

### When Investigating Production Issues
```bash
python3 scripts/diagnose_pipeline.py
```

### During Development
```bash
# Test specific functionality
python3 scripts/run_tests.py --specific tests/integration/test_processing_pipeline.py::TestProcessingPipeline::test_file_upload_and_initial_status

# Enable debug logging
LOG_LEVEL=DEBUG python3 scripts/diagnose_pipeline.py
```

## Advanced Usage

### Testing with Real GPU (if available)
```bash
unset MOCK_GPU
CUDA_VISIBLE_DEVICES=0 python3 scripts/run_tests.py --skip-diagnostics
```

### Running Individual Test Methods
```bash
python3 -m pytest tests/integration/test_processing_pipeline.py::TestProcessingPipeline::test_concurrent_job_processing -v -s
```

### Continuous Integration
All tests are designed for CI/CD:
```yaml
# GitHub Actions example
- name: Run Pipeline Tests
  run: python3 scripts/run_tests.py
```

## Interpreting Test Output

### Status Indicators
- âœ… **PASS** - Component working correctly
- âŒ **FAIL** - Issue found, check detailed output
- âš ï¸ **WARNING** - Non-critical issue
- â„¹ï¸ **INFO** - Informational message

### Key Sections to Watch
- **Database consistency** - Should show matching counts across tables
- **Queue transitions** - Jobs should move through states correctly
- **Progress updates** - Should show real-time percentage updates
- **API responses** - All endpoints should return 200 OK
- **Error handling** - Failed operations should report proper error messages

## Performance Considerations

- Tests use small audio files (1-2 seconds) for speed
- Mocked services run quickly without GPU overhead
- Database operations use optimized queries
- Tests have appropriate timeouts to prevent hanging
- Parallel test execution supported where possible

## Security Notes

- Tests use temporary sessions with no real user data
- File uploads isolated to temporary directories
- Database operations are automatically cleaned up
- No external network calls during tests
- Mocked services prevent access to real ML models

## Getting Help

### After Running Diagnostics
1. Look for âŒ symbols in output
2. Check "Recommendations" section at the end
3. Review specific error messages provided

### For Persistent Issues
1. Run with debug logging: `LOG_LEVEL=DEBUG python3 scripts/diagnose_pipeline.py`
2. Check application logs: `tail -f logs/securetranscribe.log`
3. Run specific failing tests with verbose output: `python3 -m pytest -v -s`

## Contributing Tests

When adding new tests:
1. Use descriptive names indicating what's being tested
2. Include proper cleanup to prevent pollution
3. Mock external dependencies for environment independence
4. Test both success and failure scenarios
5. Verify database state consistency
6. Add progress tracking verification for status-related tests

This comprehensive testing suite should help identify and resolve the status tracking issues you're experiencing with the transcription pipeline, providing clear feedback about where the process might be failing and verifying that all components work together correctly.